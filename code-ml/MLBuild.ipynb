{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1(a) Overview of Dataset Structure\n",
    "\n",
    "To address this part, I will use HDFS and Spark commands to:\n",
    "- List all relevant files and directories in the dataset storage locations.\n",
    "- Check file sizes and formats (CSV, TSV, ARFF, etc.).\n",
    "- Inspect file headers to determine data types (numeric, string, etc.).\n",
    "- Summarise how each dataset is stored and organised for downstream processing.\n",
    "\n",
    "This approach ensures a clear understanding of the data landscape before any loading or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 47998b5 (Standardise and enforce PAT Headers for all Q1/Q2 code cells (British English))
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud ###\n",
    "\n",
    "The credit card fraud dataset is relatively simple, with numeric features that have been anonymised using PCA. There is however significant class imbalance with only 492 examples of fraud out of 284,807 transactions in total. This requires careful handling and makes evaluating the performance of the model hard.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Splitting](#Splitting)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- You can use `sc.getConf()` to calculate the ideal number of partitions for the resources you have allocated.\n",
    "- The `randomSplit` method will return a different random split every time the output is used unless the random seed is specified or the output is cached.\n",
    "- We can write functions to extract snippets of code that we want to use more than once and we can customize their behaviour with keyword arguments.\n",
    "  - `with_custom_prediction(pred, threshold)`\n",
    "  - `show_class_balance(data, name)`\n",
    "  - `show_metrics(pred, threshold)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/09/18 11:04:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/18 11:04:49 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.\n",
      "25/09/18 11:04:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4041\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1758150289158</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-21e53659332346b9ac4e4010186f61ef</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7078</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1758150289023</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-0c38c19959ec6a34</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=4, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark imports\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports to be used locally\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(edgeitems=5, threshold=100, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bprint(\"PAT HEADER: Q1(c) Correlation Table Example\")\n",
    "# supports: Q1(c) — EDA, correlation matrix example\n",
    "# does: shows how to compute and display a correlation table for features using pandas and a colour gradient\n",
    "# Example: compute correlation matrix for features only\n",
    "# df_corr = df[[col for col in df.columns if col.startswith('V')]].corr()\n",
    "# styler = df_corr.style.applymap(color_gradient).format(\"{:.1f}\")\n",
    "# display(styler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute and display the correlation table for features (rounded, blue-white-red gradient)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Converts Spark DataFrame to pandas, computes correlation, applies style, and displays\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      5\u001b[0m df_corr \u001b[38;5;241m=\u001b[39m df[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m)]]\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m      6\u001b[0m styler \u001b[38;5;241m=\u001b[39m df_corr\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mapplymap(color_gradient)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "bprint(\"PAT HEADER: Q1(c) Correlation Table Display\")\n",
    "# supports: Q1(c) — EDA, correlation matrix display\n",
    "# does: converts Spark DataFrame to pandas, computes correlation, applies style, and displays\n",
    "df = data.toPandas()\n",
    "df_corr = df[[col for col in df.columns if col.startswith('V')]].corr()\n",
    "styler = df_corr.style.applymap(color_gradient).format(\"{:.1f}\")\n",
    "display(styler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"PAT HEADER: Q1(c) Correlation Table Helper\")\n",
    "# supports: Q1(c) — EDA, correlation matrix visualisation helper\n",
    "# does: defines color_gradient for blue-white-red background in pandas Styler\n",
    "def color_gradient(val, vmin=-1, vmax=1):\n",
    "    \"\"\"Apply a blue-white-red background colour gradient to a correlation value.\"\"\"\n",
    "    from matplotlib import colors\n",
    "    norm = (val - vmin) / (vmax - vmin)\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"bwr\", [\"#4575b4\", \"#ffffff\", \"#d73027\"])\n",
    "    rgb = cmap(norm)\n",
    "    return f\"background-color: rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})\"\n",
    "# Example usage for a pandas correlation DataFrame:\n",
    "# styler = df_corr.style.applymap(color_gradient).format(\"{:.1f}\")\n",
    "# display(styler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# (No correlation table or color_gradient code here; see dedicated cell above)\n",
    "\n",
    "def show_class_balance(data, name=\"data\", labelCol=\"label\"):\n",
    "    \"\"\"Helper function to show class balance based on label.\n",
    "    \n",
    "    Note that this function does not return anything.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): datafame with label\n",
    "        name (str): name to print above metrics for readability \n",
    "        labelCol (str): label column name\n",
    "    \"\"\"\n",
    "\n",
    "    total = data.count()\n",
    "    counts = data.groupBy(labelCol).count().toPandas()\n",
    "    counts[\"ratio\"] = counts[\"count\"] / total\n",
    "\n",
    "    print(f'Class balance [{name}]')\n",
    "    print(f'')\n",
    "    print(f'total:   {total}')\n",
    "    print(f'counts:')\n",
    "    print(counts)\n",
    "    print(f'')\n",
    "\n",
    "    \n",
    "def with_custom_prediction(\n",
    "    pred,\n",
    "    threshold,\n",
    "    probabilityCol=\"probability\",\n",
    "    customPredictionCol=\"customPrediction\",\n",
    "):\n",
    "    \"\"\"Helper function to select a custom prediction column for a custom classification threshold.\n",
    "    \n",
    "    Args:\n",
    "        pred (pyspark.sql.DataFrame): datafame with column for probability \n",
    "        threshold (float): classification threshold\n",
    "        probabilityCol (str): probability column name\n",
    "        customPredictionCol (str): new custom prediction column name\n",
    "    \n",
    "    Returns:\n",
    "        pred (pyspark.sql.DataFrame): dataframe with new colum for custom prediction\n",
    "    \"\"\"\n",
    "\n",
    "    classification_udf = F.udf(lambda x: int(x[1] > threshold), IntegerType())\n",
    "    \n",
    "    return pred.withColumn(customPredictionCol, classification_udf(F.col(probabilityCol)))\n",
    "\n",
    "\n",
    "def show_metrics(\n",
    "    pred,\n",
    "    name=\"data\",\n",
    "    threshold=0.5,\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "):\n",
    "    \"\"\"Helper function to evaluate and show metrics based on a custom classification threshold.\n",
    "    \n",
    "    Note that this function does not return anything.\n",
    "    \n",
    "    Args:\n",
    "        pred (pyspark.sql.DataFrame): datafame with column for probability \n",
    "        name (str): name to print above metrics for readability \n",
    "        threshold (float): classification threshold (default: 0.5)\n",
    "        predictionCol (str): prediction column name\n",
    "        rawPredictionCol (str): raw prediction column name\n",
    "        probabilityCol (str): probability column name\n",
    "    \"\"\"\n",
    "\n",
    "    if threshold != 0.5:\n",
    "\n",
    "        predictionCol = \"customPrediction\"\n",
    "        pred = with_custom_prediction(pred, threshold, probabilityCol=probabilityCol, customPredictionCol=predictionCol)\n",
    "\n",
    "    total = pred.count()\n",
    "\n",
    "    nP_actual = pred.filter((F.col(labelCol) == 1)).count()\n",
    "    nN_actual = pred.filter((F.col(labelCol) == 0)).count()\n",
    "\n",
    "    nP = pred.filter((F.col(predictionCol) == 1)).count()\n",
    "    nN = pred.filter((F.col(predictionCol) == 0)).count()\n",
    "    TP = pred.filter((F.col(predictionCol) == 1) & (F.col(labelCol) == 1)).count()\n",
    "    FP = pred.filter((F.col(predictionCol) == 1) & (F.col(labelCol) == 0)).count()\n",
    "    FN = pred.filter((F.col(predictionCol) == 0) & (F.col(labelCol) == 1)).count()\n",
    "    TN = pred.filter((F.col(predictionCol) == 0) & (F.col(labelCol) == 0)).count()\n",
    "\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = (TP + TN) / total\n",
    "\n",
    "    binary_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol=rawPredictionCol,\n",
    "        labelCol=labelCol,\n",
    "        metricName='areaUnderROC',\n",
    "    )\n",
    "    auroc = binary_evaluator.evaluate(pred)\n",
    "\n",
    "    print(f'Metrics [{name}]')\n",
    "    print(f'')\n",
    "    print(f'threshold: {threshold}')\n",
    "    print(f'')\n",
    "    print(f'total:     {total}')\n",
    "    print(f'')\n",
    "    print(f'nP actual: {nP_actual}')\n",
    "    print(f'nN actual: {nN_actual}')\n",
    "    print(f'')\n",
    "    print(f'nP:        {nP}')\n",
    "    print(f'nN:        {nN}')\n",
    "    print(f'')\n",
    "    print(f'TP         {TP}')\n",
    "    print(f'FP         {FP}')\n",
    "    print(f'FN         {FN}')\n",
    "    print(f'TN         {TN}')\n",
    "    print(f'')\n",
    "    print(f'precision: {precision:.8f}')\n",
    "    print(f'recall:    {recall:.8f}')\n",
    "    print(f'accuracy:  {accuracy:.8f}')\n",
    "    print(f'')\n",
    "    print(f'auroc:     {auroc:.8f}')\n",
    "\n",
    "\n",
    "def expand(x, s=0.05, d=0):\n",
    "    \"\"\"Expand a two element array about its center point by a relative scale or a fixed offset.\n",
    "    Args:\n",
    "        x (list|np.array): two element array\n",
    "        s (float): relative scale to expand array based on its width x[1] - x[0]\n",
    "        d (float): fixed offset to expand array\n",
    "    Returns:\n",
    "        x (np.array): expanded two element array\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(x)\n",
    "    d = d + s * (x[1] - x[0])\n",
    "    \n",
    "    return x + np.array([-d, d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ideal number of partitions\n",
    "\n",
    "conf = sc.getConf()\n",
    "\n",
    "N = int(conf.get(\"spark.executor.instances\"))\n",
    "M = int(conf.get(\"spark.executor.cores\"))\n",
    "partitions = 4 * N * M\n",
    "\n",
    "print(f'ideal # partitions = {partitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "The credit card fraud dataset is stored in HDFS.\n",
    "\n",
    "We will load the dataset and then use `VectorAssembler` to combine the separate feature columns into the single vector column that is expected by most of the classes in the machine learning. \n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data in gzip compressed but Spark will automatically uncompress it as it is loaded.\n",
    "- The data has a header so we can infer schema conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from IPython.display     import display  # calls between environments\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame\n",
    "from pyspark.sql         import DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from typing              import List, Optional, Tuple\n",
    "\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import math, os, platform, re\n",
    "import subprocess, sys, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark → pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "    \n",
    "def show_df(df, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    bprint()\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "  \n",
    "def write_parquet(df, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[cathch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "  \n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "    \n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe …\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    " \n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    " \n",
    "def probe_universe(daily_df, stations_df, inv_agg_df, tag=\"\"):\n",
    "    \"\"\"\n",
    "    DIAGNOSTIC\n",
    "    \"\"\"\n",
    "    # quick previews\n",
    "    daily_df.show(20)\n",
    "    stations_df.show(20)\n",
    "    inv_agg_df.show(20)\n",
    "\n",
    "    print(tag)\n",
    "    daily_df.printSchema()\n",
    "    stations_df.printSchema()\n",
    "    inv_agg_df.printSchema()\n",
    "    print(tag)\n",
    "\n",
    "    print(\"\\n\" + \"_\"*70)\n",
    "    print(f\"[PROBE] Station universe check :: {tag}\")\n",
    "\n",
    "    # id universes\n",
    "    daily_ids   = _ids(daily_df)\n",
    "    station_ids = _ids(stations_df)\n",
    "    inv_ids     = _ids(inv_agg_df)\n",
    "\n",
    "    # counts\n",
    "    print(\"[COUNT] daily IDs         :\", daily_ids.count())\n",
    "    print(\"[COUNT] station IDs (cat) :\", station_ids.count())\n",
    "    print(\"[COUNT] inventory IDs     :\", inv_ids.count())\n",
    "\n",
    "    # set differences\n",
    "    print(\"[DIFF ] daily - station   :\", daily_ids.join(station_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station - daily   :\", station_ids.join(daily_ids, \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] station - inv     :\", station_ids.join(inv_ids,    \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv - daily       :\", inv_ids.join(daily_ids,      \"ID\", \"left_anti\").count())\n",
    "    print(\"[DIFF ] inv - station     :\", inv_ids.join(station_ids,    \"ID\", \"left_anti\").count())\n",
    "\n",
    "    bprint(\"[done] probe_universe\")\n",
    "\n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    "def pick_unfiltered_daily(preferred_path: str = None) -> DataFrame:\n",
    "    \"\"\"Return an unfiltered daily DF (~129k unique station IDs).\"\"\"\n",
    "    cand_names = [\"daily\", \"read_daily\", \"daily_df\", \"daily_all\", \"ghcnd_daily\"]\n",
    "    print(\"[INFO] Candidate DataFrames:\", [n for n in cand_names if n in globals()])\n",
    "    for name in cand_names:\n",
    "        obj = globals().get(name)\n",
    "        if isinstance(obj, DataFrame):\n",
    "            try:\n",
    "                n = normalise_ids(obj).count()\n",
    "                print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {name} as the unfiltered daily.\")\n",
    "                    return obj\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not inspect {name}:\", repr(e))\n",
    "    if preferred_path:\n",
    "        print(f\"[INFO] Trying preferred_path: {preferred_path}\")\n",
    "        df = spark.read.parquet(str(preferred_path))\n",
    "        n = normalise_ids(df).count()\n",
    "        print(\"[CHECK] preferred_path unique station IDs:\", n)\n",
    "        if n >= 120_000:\n",
    "            print(\"[INFO] Using preferred_path as the unfiltered daily.\")\n",
    "            return df\n",
    "    for var in [\"DAILY_READ_NAME\",\"DAILY_WRITE_NAME\",\"daily_read_name\",\"daily_write_name\",\"DAILY_NAME\"]:\n",
    "        if var in globals():\n",
    "            path = globals()[var]\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} = {path}\")\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = normalise_ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {var} as the unfiltered daily.\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", repr(e))\n",
    "    raise SystemExit(\"[FATAL] Could not find an unfiltered daily dataset (expected ~129k unique station IDs).\")\n",
    "\n",
    "def bprint(text: str=\"\", l=50):\n",
    "    n = len(text)\n",
    "    n = abs(n - l)//2\n",
    "    \n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming un-convention\n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    " \n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dφ, dλ = (φ2 - φ1), (λ2 - λ1)\n",
    "            a = sin(dφ/2)**2 + cos(φ1)*cos(φ2)*sin(dλ/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(φ1)*sin(φ2) + cos(φ1)*cos(φ2)*cos(λ2 - λ1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealand’s latitudes (≈36–47°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37°S):       ~6,370.4 km\n",
    "    Christchurch (43.5°S): ~6,368.0 km\n",
    "    Dunedin (45.9°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    Wellington (41°S):     ~6,369.0 km\n",
    "    mean                  ≈ 6,368.6 km\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav    = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 1: ENVIRONMENT SETUP\")\n",
    "# supports: Assignment Setup — \"Configure global variables and paths required for GHCN data processing\"\n",
    "# does: initializes runtime tracking, configures Azure storage paths, defines data locations,\n",
    "#       establishes file paths, and sets debug flags for conditional processing\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "val               = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "bprint()\n",
    "print(f\"[time] current time           :  {val}\")\n",
    "bprint()\n",
    "\n",
    "bprint(\"ENVIRONMENT\") \n",
    "print(\"Spark       :\", spark.version)\n",
    "print(\"Python tuple:\", sys.version_info[:3]) \n",
    "print(\"username    :\", username)\n",
    "print()\n",
    "\n",
    "bprint(\"DEEBUG BOOLEANS\")\n",
    "#FORCE_OVERWRITE = False  # False means that if the file exists then we wont re-write it \n",
    "#FORCE_OVERWRITE = True   # True means overwrite all resultant files\n",
    "FORCE_REBUILD_ENRICHED  = True   #has_parquet(enriched_write_name)\n",
    "FORCE_REBUILD_INV_AGG = True    # has_parquet(inv_agg_write_name)\n",
    "\n",
    "FORCE_REBUILD_STATIONS  = True    #has_parquet(stations_write_name)\n",
    "FORCE_REBUILD_INVENTORY = True    # has_parquet(inventory_write_name)\n",
    "FORCE_REBUILD_STATES    = True    #has_parquet(states_write_name)\n",
    "FORCE_REBUILD_COUNTRIES = True    #has_parquet(countries_write_name)\n",
    "FORCE_REBUILD_INV_AGG = True    # has_parquet(inv_agg_write_name)\n",
    "\n",
    "FORCE_REBUILD_OVERLAP   = True    #has_parquet(overlap_write_name)\n",
    "FORCE_REBUILD_PRECIP    = True    #has_parquet(precip_write_path)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_ENRICHED  :\", FORCE_REBUILD_ENRICHED)\n",
    "print(f\"[status] FORCE_REBUILD_INV_AGG   :\", FORCE_REBUILD_INV_AGG)\n",
    "print(f\"[status] FORCE_REBUILD_STATIONS  :\", FORCE_REBUILD_STATIONS)\n",
    "print(f\"[status] FORCE_REBUILD_INVENTORY :\", FORCE_REBUILD_INVENTORY)\n",
    "print(f\"[status] FORCE_REBUILD_STATES    :\", FORCE_REBUILD_STATES)\n",
    "print(f\"[status] FORCE_REBUILD_COUNTRIES :\", FORCE_REBUILD_COUNTRIES)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_OVERLAP   :\", FORCE_REBUILD_OVERLAP)\n",
    "print(f\"[status] FORCE_REBUILD_PRECIP    :\", FORCE_REBUILD_PRECIP)\n",
    "\n",
    "bprint(\"SOURCE FOLDERS\")\n",
    "print()\n",
    "\n",
    "azure_account_name        = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "previous_year             = 2024  # full hear\n",
    "most_recent_year          = 2025  # currently building\n",
    "\n",
    "\n",
    "print(\"azure_account_name        :\", azure_account_name)\n",
    "print(\"azure_data_container_name :\", azure_data_container_name)\n",
    "print(\"azure_user_container_name :\", azure_user_container_name)\n",
    "print(\"previous_year             :\", previous_year)\n",
    "print(\"most_recent_year          :\", most_recent_year)\n",
    "print()\n",
    "\n",
    "data_root      = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\"\n",
    "user_root      = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/\"\n",
    " \n",
    "data_root      = ensure_dir(data_root)\n",
    "user_root      = ensure_dir(user_root) \n",
    "\n",
    "print(\"data_root           :\", data_root) \n",
    "print(\"user_root           :\", user_root)\n",
    "bprint()\n",
    "\n",
    "daily_root     = ensure_dir(f\"{data_root}daily/\")\n",
    "\n",
    "print(\"daily_root          :\", daily_root)\n",
    "print()\n",
    "\n",
    "aux_root = \"../auxiliary/\"\n",
    "aux_root = ensure_dir(aux_root)\n",
    "\n",
    "reports_dir  = ensure_dir(f\"{aux_root}reports/\")\n",
    "images_dir   = ensure_dir(f\"{aux_root}images/\")\n",
    "figures_dir  = ensure_dir(f\"{aux_root}figures/\") \n",
    "\n",
    "\n",
    "print(\"aux_root    :\", aux_root)\n",
    "print(\"reports_dir :\", reports_dir)\n",
    "print(\"images_dir  :\", images_dir)\n",
    "print(\"figures_dir :\", figures_dir)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "bprint(\"SOURCE FILES\")\n",
    "stations_read_name   = f'{data_root}ghcnd-stations.txt'\n",
    "inventory_read_name  = f'{data_root}ghcnd-inventory.txt'\n",
    "countries_read_name  = f'{data_root}ghcnd-countries.txt'\n",
    "states_read_name     = f'{data_root}ghcnd-states.txt'\n",
    "\n",
    "\n",
    "print(\"stations_read_name  :\", stations_read_name)\n",
    "print(\"inventory_read_name :\", inventory_read_name)\n",
    "print(\"countries_read_name :\", countries_read_name)\n",
    "print(\"states_read_name    :\", states_read_name)\n",
    "print()\n",
    "\n",
    "previous_csvgz_path  = f'{daily_root}2024.csv.gz' \n",
    "current_csvgz_path   = f'{daily_root}2025.csv.gz' \n",
    "\n",
    "\n",
    "print(\"previous_csvgz_path  :\", previous_csvgz_path)\n",
    "print(\"current_csvgz_path   :\", current_csvgz_path)\n",
    "print()\n",
    "bprint(\"USER FOLDERS\")\n",
    "  \n",
    "stations_write_name  =  ensure_dir(f'{user_root}stations.parquet')      #parquest file referenced by folder\n",
    "inventory_write_name =  ensure_dir(f'{user_root}inventory.parquet')\n",
    "countries_write_name =  ensure_dir(f'{user_root}countries.parquet')\n",
    "states_write_name    =  ensure_dir(f'{user_root}states.parquet') \n",
    "\n",
    "inv_agg_write_name   = ensure_dir(f'{user_root}inv_agg.parquet')\n",
    "enriched_write_name  = ensure_dir(f'{user_root}enriched_write_name.parquet')\n",
    "\n",
    "print()\n",
    "print(\"stations_write_name  :\", stations_write_name)\n",
    "print(\"inventory_write_name :\", inventory_write_name)\n",
    "print(\"countries_write_name :\", countries_write_name)\n",
    "print(\"states_write_name    :\", states_write_name)\n",
    "print(\"inv_agg_write_name   :\", inv_agg_write_name)\n",
    "print()\n",
    "print(\"enriched_write_name :\", enriched_write_name)\n",
    "print(\"enriched_write_name :\", enriched_write_name)\n",
    "\n",
    "#overlap_write_pathh  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "#precip_write_path    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet')\n",
    " \n",
    "print(\"inv_agg_write_name  :\", inv_agg_write_name)\n",
    "station_date_element = ensure_dir(f\"{user_root}q2a_station_date_element.parquet\")\n",
    "overlap_counts_name  = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "overlap_write_name   = ensure_dir(f'{user_root}q1b32_overlap_counts.parquet')\n",
    "precip_write_name    = ensure_dir(f'{user_root}q2a-agg-precipitation.parquet') \n",
    "print()\n",
    "\n",
    "inv_agg_write_name   = ensure_dir(f'{user_root}inv_agg.parquet') \n",
    "\n",
    "print()\n",
    "print(\"stations_write_name :\", stations_write_name)\n",
    "print(\"overlap_counts_name :\", overlap_counts_name)\n",
    "print(\"overlap_write_name  :\", overlap_write_name)\n",
    "print(\"precip_write_name   :\", precip_write_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(a) List files and inspect dataset structure in Azure Data Lake\n",
    "azure_account_name = 'madsstorage002'\n",
    "azure_data_container_name = 'campus-data'\n",
    "azure_user_container_name = 'campus-user'\n",
    "username = 'USERNAME'  # TODO: Replace with your username\n",
    "\n",
    "# Example: List files in the user container (adjust path as needed)\n",
    "data_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/creditcard-fraud/'\n",
    "\n",
    "# List files in the directory\n",
    "dbutils.fs.ls(data_path)\n",
    "\n",
    "# Check file sizes and preview headers\n",
    "file_to_preview = data_path + 'creditcard.csv'  # Adjust filename as needed\n",
    "df_preview = spark.read.option('header', True).csv(file_to_preview)\n",
    "df_preview.show(5)\n",
    "df_preview.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST ASSIGNMENT CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from HDFS\n",
    "\n",
    "fraud = (\n",
    "    spark.read.csv(\"hdfs:///data/fraud/manipulated.csv.gz\", header=True, inferSchema=True)\n",
    "    .repartition(partitions)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "fraud.printSchema()\n",
    "show_as_html(fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select what we need\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col for col in fraud.columns if col.startswith(\"V\")],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(fraud)\n",
    "data = data.select(\n",
    "    F.col('features'),\n",
    "    F.col('Class').alias('label'),\n",
    ")\n",
    "\n",
    "data.printSchema()\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the correlation table for features (rounded, blue-white-red gradient)\n",
    "# Converts Spark DataFrame to pandas, computes correlation, applies style, and displays\n",
    "\n",
    "try:\n",
    "    df = data.toPandas()\n",
    "    df_corr = df[[col for col in df.columns if col.startswith('V')]].corr()\n",
    "    styler = df_corr.style.applymap(color_gradient).format(\"{:.1f}\")\n",
    "    display(styler)\n",
    "except NameError as e:\n",
    "    print(\"Data is not defined. Please run the previous cells that load and prepare the data.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting ###\n",
    "\n",
    "There are several methods for random splitting, but not all of them are as random as might be assumed.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- `randomSplit` does simple random sampling only\n",
    "- `sampleBy` does class specific simple random sampling where the proportion of each class can be changed\n",
    "- `Window` functions can be used to achieve exact stratification if required\n",
    "  - We can also use Python control structures to make it easier to implement complicated logic such as the filtering below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"PAT HEADER: Q2(a) Random Split\")\n",
    "# supports: Q2(a) — randomSplit for train/test\n",
    "# does: splits data into training and test sets using randomSplit, caches, and shows class balance\n",
    "training, test = data.randomSplit([0.8, 0.2])\n",
    "training.cache()\n",
    "test.cache()\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"PAT HEADER: Q2(b) SampleBy Split\")\n",
    "# supports: Q2(b) — sampleBy for per-class split\n",
    "# does: splits data using sampleBy for each class, caches, and shows class balance\n",
    "temp = data.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "training = temp.sampleBy(\"label\", fractions={0: 0.8, 1: 0.8})\n",
    "training.cache()\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "training = training.drop(\"id\")\n",
    "test = test.drop(\"id\")\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"PAT HEADER: Q2(c) Window Stratification (Binary)\")\n",
    "# supports: Q2(c) — exact stratification using Window (binary)\n",
    "# does: splits data using row_number for each class, caches, and shows class balance\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"random\", F.rand())\n",
    "    .withColumn(\"row\", F.row_number().over(Window.partitionBy(\"label\").orderBy(\"random\")))\n",
    ")\n",
    "training = temp\n",
    "training = training.where((F.col(\"label\") != 0) | (F.col(\"row\") < 284315 * 0.8))\n",
    "training = training.where((F.col(\"label\") != 1) | (F.col(\"row\") < 492    * 0.8))\n",
    "training.cache()\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "training = training.drop(\"id\", \"random\", \"row\")\n",
    "test = test.drop(\"id\", \"random\", \"row\")\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"PAT HEADER: Q2(d) Window Stratification (Multi-class)\")\n",
    "# supports: Q2(d) — exact stratification using Window (multi-class)\n",
    "# does: splits data using row_number for each class (counts auto), caches, and shows class balance\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"random\", F.rand())\n",
    "    .withColumn(\"row\", F.row_number().over(Window.partitionBy(\"label\").orderBy(\"random\")))\n",
    ")\n",
    "counts = {label: count for label, count in data.groupBy(\"label\").count().collect()}\n",
    "training = temp\n",
    "for label, count in counts.items():\n",
    "    training = training.where((F.col(\"label\") != label) | (F.col(\"row\") < count * 0.8))\n",
    "training.cache()\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "training = training.drop(\"id\", \"random\", \"row\")\n",
    "test = test.drop(\"id\", \"random\", \"row\")\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"PAT HEADER: Q2(e) Window Stratification (Percent Rank)\")\n",
    "# supports: Q2(e) — exact stratification using Window (percent rank)\n",
    "# does: splits data using percent_rank for each class, caches, and shows class balance\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"random\", F.rand())\n",
    "    .withColumn(\"rank\", F.percent_rank().over(Window.partitionBy(\"label\").orderBy(\"random\")))\n",
    ")\n",
    "training = temp\n",
    "for label, count in counts.items():\n",
    "    training = training.where((F.col(\"label\") != label) | (F.col(\"rank\") < 0.8))\n",
    "training.cache()\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "training = training.drop(\"id\", \"random\", \"row\")\n",
    "test = test.drop(\"id\", \"random\", \"row\")\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Method Overview for All Questions (Q1–Q6)\n",
    "\n",
    "## Q1(a) Overview of Dataset Structure\n",
    "- Use HDFS and Spark commands to:\n",
    "  - List all relevant files and directories in dataset storage locations\n",
    "  - Check file sizes and formats (CSV, TSV, ARFF, etc.)\n",
    "  - Inspect file headers to determine data types (numeric, string, etc.)\n",
    "  - Summarise how each dataset is stored and organised for downstream processing\n",
    "- Ensures a clear understanding of the data landscape before any loading or analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Processing Answer: Q1(a)01\")\n",
    "# supports: Q1(a) — list files, check formats, inspect headers\n",
    "# does: lists files in root_data, previews file headers, and prints storage summary\n",
    "\n",
    "def list_hdfs_files(path):\n",
    "    print(f\"Listing files in: {path}\")\n",
    "    files = os.popen(f'hdfs dfs -ls \"{path}\"').read()\n",
    "    print(files)\n",
    "\n",
    "# Example: list files in the main data directory\n",
    "data_dir = root_data  # defined in previous cell\n",
    "list_hdfs_files(data_dir)\n",
    "\n",
    "# Check file formats and preview headers\n",
    "for fname in [stations_read_name, inventory_read_name, countries_read_name, states_read_name]:\n",
    "    print(f\"\\nPreview of {fname}:\")\n",
    "    lines = os.popen(f'hdfs dfs -cat \"{fname}\" | head -5').read()\n",
    "    print(lines)\n",
    "\n",
    "# Summarise storage for downstream processing\n",
    "print(\"\\nData root:\", root_data)\n",
    "print(\"User root:\", root_user)\n",
    "# (Add any additional summary as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q1(b) Data Loading and Preparation\n",
    "- Load the credit card fraud dataset from HDFS using Spark's CSV reader (header, infer schema)\n",
    "- Repartition and cache the DataFrame for efficient distributed processing\n",
    "- Use `VectorAssembler` to combine feature columns into a single vector column for Spark ML\n",
    "- Select only the features and label columns for downstream modelling\n",
    "- Print the schema and show a sample of the data to confirm correct loading and structure\n",
    "- Ensures the data is ready for machine learning workflows and further analysis\n",
    "\n",
    "## Q1(c) Exploratory Data Analysis (EDA)\n",
    "- Compute summary statistics for all features (mean, std, min, max, etc.)\n",
    "- Visualise class balance and feature distributions using:\n",
    "  - Histograms\n",
    "  - Boxplots\n",
    "- Compute and display the correlation matrix for features\n",
    "  - Use a blue-white-red colour gradient for clarity\n",
    "- Identify missing values or anomalies in the data\n",
    "- Provides insight into the data's structure, relationships, and potential issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"Processing Answer: Q1(b)02\")\n",
    "# supports: Q1(b) — load and prepare the credit card fraud data\n",
    "# does: loads data from HDFS, repartitions, caches, assembles features, and selects label column\n",
    "\n",
    "# Load the dataset from HDFS (CSV, header, infer schema)\n",
    "fraud = (\n",
    "    spark.read.csv(f\"{daily_root}2025.csv.gz\", header=True, inferSchema=True)\n",
    "    .repartition(partitions)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"[INFO] Loaded fraud data from:\", f\"{daily_root}2025.csv.gz\")\n",
    "fraud.printSchema()\n",
    "fraud.show(5)\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "feature_cols = [col for col in fraud.columns if col.startswith(\"V\")]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(fraud)\n",
    "data = data.select(\n",
    "    \"features\",\n",
    "    F.col(\"Class\").alias(\"label\")\n",
    ")\n",
    "\n",
    "print(\"[INFO] Assembled features and selected label column.\")\n",
    "data.printSchema()\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q2(a) Data Splitting: Random Split\n",
    "- Use Spark's `randomSplit` to divide the data into training and test sets (e.g., 80/20 split)\n",
    "- Set a random seed for reproducibility\n",
    "- Cache and persist the resulting splits for efficient reuse\n",
    "- Show class balance in each split to ensure representativeness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q3(a) Feature Engineering: Scaling and Transformation\n",
    "- Apply scaling (e.g., StandardScaler, MinMaxScaler) to numeric features\n",
    "- Transform skewed features using log or Box-Cox transformations\n",
    "- Document the rationale and effect of each transformation\n",
    "- Assess impact on model performance and interpretability\n",
    "\n",
    "## Q3(b) Feature Engineering: Encoding and Construction\n",
    "- Encode categorical variables using one-hot or ordinal encoding\n",
    "- Create new features from existing ones (e.g., ratios, interactions, polynomials)\n",
    "- Use domain knowledge to guide feature construction\n",
    "- Evaluate new features with exploratory analysis and model validation\n",
    "\n",
    "## Q3(c) Feature Engineering: Feature Selection\n",
    "- Use filter, wrapper, and embedded methods to select important features:\n",
    "  - Correlation analysis\n",
    "  - Recursive feature elimination\n",
    "  - Feature importances from tree-based models\n",
    "- Compare model performance before and after selection\n",
    "- Justify the final set of features used\n",
    "\n",
    "## Q4(a) Model Training: Logistic Regression\n",
    "- Train a logistic regression model using the training data\n",
    "- Tune hyperparameters (e.g., regularisation strength, max iterations)\n",
    "- Use cross-validation to select the best model\n",
    "- Save the trained model for later use\n",
    "\n",
    "## Q4(b) Model Training: Tree-based Models\n",
    "- Train decision tree and random forest classifiers\n",
    "- Optimise tree depth, number of trees, and other hyperparameters\n",
    "- Use feature importances for interpretation\n",
    "- Compare performance to logistic regression\n",
    "\n",
    "## Q4(c) Model Training: Gradient Boosting\n",
    "- Train a gradient boosting machine (e.g., GBTClassifier)\n",
    "- Tune learning rate, number of iterations, and tree parameters\n",
    "- Use early stopping to prevent overfitting\n",
    "- Summarise results and compare to other models\n",
    "\n",
    "## Q4(d) Model Evaluation and Comparison\n",
    "- Evaluate all models using consistent metrics (accuracy, precision, recall, AUROC)\n",
    "- Summarise results in tables and plots\n",
    "- Discuss strengths and weaknesses of each model\n",
    "- Select the best model based on validation metrics and interpretability\n",
    "\n",
    "## Q5(a) Deployment Planning\n",
    "- Prepare the trained model for deployment:\n",
    "  - Serialise and export the model in a portable format\n",
    "  - Integrate with data pipelines for automated scoring\n",
    "  - Develop API endpoints or batch processing scripts\n",
    "- Document deployment requirements and dependencies\n",
    "- Plan for scaling and resource management in production\n",
    "\n",
    "## Q5(b) Automation and Reproducibility\n",
    "- Develop scripts or notebooks to automate:\n",
    "  - Data ingestion\n",
    "  - Model training and evaluation\n",
    "  - Reporting and visualisation\n",
    "- Ensure all steps are reproducible and well-documented\n",
    "- Use version control for code and data\n",
    "- Provide workflow diagrams or Makefiles for end-to-end automation\n",
    "\n",
    "## Q5(c) Monitoring and Maintenance\n",
    "- Set up monitoring for model performance and data drift:\n",
    "  - Track key metrics over time\n",
    "  - Implement alerting for performance degradation\n",
    "- Schedule regular retraining and validation\n",
    "- Document maintenance procedures and responsibilities\n",
    "- Plan for model versioning and rollback\n",
    "\n",
    "## Q6(a) Reflection: Strengths and Limitations\n",
    "- Reflect on the strengths and limitations of the overall approach:\n",
    "  - Data quality and representativeness\n",
    "  - Model assumptions and limitations\n",
    "  - Computational efficiency\n",
    "- Suggest improvements for future work:\n",
    "  - Collecting more data\n",
    "  - Trying advanced models or ensembles\n",
    "  - Improving feature engineering\n",
    "- Discuss lessons learned and unexpected challenges\n",
    "\n",
    "## Q6(b) Recommendations and Communication\n",
    "- Summarise key findings and actionable recommendations for stakeholders:\n",
    "  - Model performance and reliability\n",
    "  - Next steps for deployment or further analysis\n",
    "  - Communication of results to technical and non-technical audiences\n",
    "- Prepare executive summaries and technical documentation\n",
    "- Propose a roadmap for future enhancements\n",
    "\n",
    "## Q6(c) Ethical and Practical Considerations\n",
    "- Discuss ethical implications of model deployment (e.g., fairness, bias, privacy)\n",
    "- Address practical challenges in real-world use (e.g., interpretability, scalability)\n",
    "- Propose strategies to mitigate risks and ensure responsible AI practices\n",
    "- Review compliance with relevant regulations and standards\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook follows a modular, reproducible workflow using Spark and pandas, with visualisation and helper functions to support robust analysis and reporting for all assignment questions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
