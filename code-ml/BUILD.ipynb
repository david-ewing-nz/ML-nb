{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b3e44a",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\"\"\"\n",
    "    html = []\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "    return ''.join(html)\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\"\"\"\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\"\"\"\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "    else:\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012747e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d# Spark imports\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab77a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color gradient function for correlation tables\n",
    "def color_gradient(val):\n",
    "    \"\"\"Return color for value: blue for positive, red for negative, white for zero.\"\"\"\n",
    "    from matplotlib import colors\n",
    "    cmap = colors.LinearSegmentedColormap.from_list('', ['red','white','blue'])\n",
    "    norm = colors.Normalize(vmin=-1, vmax=1)\n",
    "    rgb = cmap(norm(val))[:3]\n",
    "    return f'background-color: rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c61020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark â†’ pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "    \n",
    "def show_df(df: DataFrame, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    bprint()\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "  \n",
    "def write_parquet(df: DataFrame, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[cathch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "  \n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "     \n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe â€¦\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    " \n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "def probe_univ \n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    "def pick_unfiltered_daily(preferred_path: str = None) -> DataFrame:\n",
    "    \"\"\"Return an unfiltered daily DF (~129k unique station IDs).\"\"\"\n",
    "    cand_names = [\"daily\", \"read_daily\", \"daily_df\", \"daily_all\", \"ghcnd_daily\"]\n",
    "    print(\"[INFO] Candidate DataFrames:\", [n for n in cand_names if n in globals()])\n",
    "    for name in cand_names:\n",
    "        obj = globals().get(name)\n",
    "        if isinstance(obj, DataFrame):\n",
    "            try:\n",
    "                n = normalise_ids(obj).count()\n",
    "                print(f\"[CHECK] {name} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {name} as the unfiltered daily.\")\n",
    "                    return obj\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not inspect {name}:\", repr(e))\n",
    "    if preferred_path:\n",
    "        print(f\"[INFO] Trying preferred_path: {preferred_path}\")\n",
    "        df = spark.read.parquet(str(preferred_path))\n",
    "        n = normalise_ids(df).count()\n",
    "        print(\"[CHECK] preferred_path unique station IDs:\", n)\n",
    "        if n >= 120_000:\n",
    "            print(\"[INFO] Using preferred_path as the unfiltered daily.\")\n",
    "            return df\n",
    "    for var in [\"DAILY_READ_NAME\",\"DAILY_WRITE_NAME\",\"daily_read_name\",\"daily_write_name\",\"DAILY_NAME\"]:\n",
    "        if var in globals():\n",
    "            path = globals()[var]\n",
    "            try:\n",
    "                print(f\"[INFO] Trying {var} = {path}\")\n",
    "                df = spark.read.parquet(str(path))\n",
    "                n = normalise_ids(df).count()\n",
    "                print(f\"[CHECK] {var} unique station IDs:\", n)\n",
    "                if n >= 120_000:\n",
    "                    print(f\"[INFO] Using {var} as the unfiltered daily.\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read {var}:\", repr(e))\n",
    "    raise SystemExit(\"[FATAL] Could not find an unfiltered daily dataset (expected ~129k unique station IDs).\")\n",
    "\n",
    "def bprint(text: str=\"\", l=50):\n",
    "    n = len(text)\n",
    "    n = abs(n - l)//2\n",
    "    \n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming un-convention\n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    " \n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealandâ€™s latitudes (â‰ˆ36â€“47Â°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37Â°S):       ~6,370.4 km\n",
    "    Christchurch (43.5Â°S): ~6,368.0 km\n",
    "    Dunedin (45.9Â°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    Wellington (41Â°S):     ~6,369.0 km\n",
    "    mean                  â‰ˆ 6,368.6 km\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav    = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067037e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint(\"SECTION 1: ENVIRONMENT SETUP\")\n",
    "# supports: Assignment Setup â€” \"Configure global variables and paths required for GHCN data processing\"\n",
    "# does: initializes runtime tracking, configures Azure storage paths, defines data locations,\n",
    "#       establishes file paths, and sets debug flags for conditional processing\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "val               = spark.range(1).select(F.date_format(F.current_timestamp(), 'yyyy.MM.dd HH:mm').alias('t')).first()['t']\n",
    "bprint()\n",
    "print(f\"[time] current time           :  {val}\")\n",
    "bprint()\n",
    "\n",
    "\n",
    "bprint(\"ENVIRONMENT\") \n",
    "print(\"Spark       :\", spark.version)\n",
    "print(\"Python tuple:\", sys.version_info[:3]) \n",
    "print(\"username    :\", username)\n",
    "print()\n",
    "\n",
    "bprint(\"DEEBUG LOGICALS\")\n",
    "#FORCE_OVERWRITE = False  # False means that if the file exists then we wont re-write it \n",
    "#FORCE_OVERWRITE = True   # True means overwrite all resultant files\n",
    "\n",
    "FORCE_REBUILD_ENRICHED  = False   #has_parquet(enriched_write_name)\n",
    "FORCE_REBUILD_OVERLAP   = False    #has_parquet(overlap_write_name)\n",
    "FORCE_REBUILD_PRECIP    = False    #has_parquet(precip_write_path)\n",
    "\n",
    "FORCE_REBUILD_STATIONS  = False    #has_parquet(stations_write_name)\n",
    "FORCE_REBUILD_INVENTORY = False    # has_parquet(inventory_write_name)\n",
    "FORCE_REBUILD_STATES    = False    #has_parquet(states_write_name)\n",
    "FORCE_REBUILD_COUNTRIES = False    #has_parquet(countries_write_name)\n",
    "FORCE_REBUILD_INV_AGG   = False    # has_parquet(inv_agg_write_name)\n",
    "\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_ENRICHED  :\", FORCE_REBUILD_ENRICHED)\n",
    "print(f\"[status] FORCE_REBUILD_INV_AGG   :\", FORCE_REBUILD_INV_AGG)\n",
    "print(f\"[status] FORCE_REBUILD_STATIONS  :\", FORCE_REBUILD_STATIONS)\n",
    "print(f\"[status] FORCE_REBUILD_INVENTORY :\", FORCE_REBUILD_INVENTORY)\n",
    "print(f\"[status] FORCE_REBUILD_STATES    :\", FORCE_REBUILD_STATES)\n",
    "print(f\"[status] FORCE_REBUILD_COUNTRIES :\", FORCE_REBUILD_COUNTRIES)\n",
    "\n",
    "print(f\"[status] FORCE_REBUILD_OVERLAP   :\", FORCE_REBUILD_OVERLAP)\n",
    "print(f\"[status] FORCE_REBUILD_PRECIP    :\", FORCE_REBUILD_PRECIP)\n",
    "\n",
    "bprint(\"SOURCE FOLDERS\")\n",
    "print()\n",
    "\n",
    "azure_account_name        = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "previous_year             = 2024  # full hear\n",
    "most_recent_year          = 2025  # currently building\n",
    "\n",
    "\n",
    "print(\"azure_account_name        :\", azure_account_name)\n",
    "print(\"azure_data_container_name :\", azure_data_container_name)\n",
    "print(\"azure_user_container_name :\", azure_user_container_name)\n",
    "print(\"previous_year             :\", previous_year)\n",
    "print(\"most_recent_year          :\", most_recent_year)\n",
    "print()\n",
    "\n",
    "data_root      = f\"wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\"\n",
    "user_root      = f\"wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/\"\n",
    " \n",
    "data_root      = ensure_dir(data_root)\n",
    "user_root      = ensure_dir(user_root) \n",
    "\n",
    "print(\"data_root           :\", data_root) \n",
    "print(\"user_root           :\", user_root)\n",
    "bprint()\n",
    "\n",
    "daily_root     = ensure_dir(f\"{data_root}daily/\")\n",
    "\n",
    "print(\"daily_root          :\", daily_root)\n",
    "print()\n",
    "\n",
    "aux_root = \"../auxiliary/\"\n",
    "aux_root = ensure_dir(aux_root)\n",
    "\n",
    "reports_dir  = ensure_dir(f\"{aux_root}reports/\")\n",
    "images_dir   = ensure_dir(f\"{aux_root}images/\")\n",
    "figures_dir  = ensure_dir(f\"{aux_root}figures/\") \n",
    "\n",
    "\n",
    "print(\"aux_root    :\", aux_root)\n",
    "print(\"reports_dir :\", reports_dir)\n",
    "print(\"images_dir  :\", images_dir)\n",
    "print(\"figures_dir :\", figures_dir)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
