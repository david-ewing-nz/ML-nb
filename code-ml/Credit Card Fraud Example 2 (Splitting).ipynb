{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud ###\n",
    "\n",
    "The credit card fraud dataset is relatively simple, with numeric features that have been anonymised using PCA. There is however significant class imbalance with only 492 examples of fraud out of 284,807 transactions in total. This requires careful handling and makes evaluating the performance of the model hard.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Splitting](#Splitting)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- You can use `sc.getConf()` to calculate the ideal number of partitions for the resources you have allocated.\n",
    "- The `randomSplit` method will return a different random split every time the output is used unless the random seed is specified or the output is cached.\n",
    "- We can write functions to extract snippets of code that we want to use more than once and we can customize their behaviour with keyword arguments.\n",
    "  - `with_custom_prediction(pred, threshold)`\n",
    "  - `show_class_balance(data, name)`\n",
    "  - `show_metrics(pred, threshold)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=4, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark imports\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports to be used locally\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(edgeitems=5, threshold=100, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display correlation table (rounded, colourised)\n",
    "# Assumes you have a pandas DataFrame of features, e.g. df = data.toPandas()\n",
    "# Replace 'df' with your DataFrame variable as needed\n",
    "\n",
    "# Example: compute correlation matrix for features only\n",
    "# df_corr = df[[col for col in df.columns if col.startswith('V')]].corr()\n",
    "# styler = df_corr.style.applymap(color_gradient).format(\"{:.1f}\")\n",
    "# display(styler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation table visualisation helpers (British English, deduplicated)\n",
    "def color_gradient(val, vmin=-1, vmax=1):\n",
    "    \"\"\"Apply a blue-white-red background colour gradient to a correlation value.\"\"\"\n",
    "    from matplotlib import colors\n",
    "    norm = (val - vmin) / (vmax - vmin)\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"bwr\", [\"#4575b4\", \"#ffffff\", \"#d73027\"])\n",
    "    rgb = cmap(norm)\n",
    "    return f\"background-color: rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})\"\n",
    "\n",
    "# Example usage for a pandas correlation DataFrame:\n",
    "# styler = df_corr.style.applymap(color_gradient).format(\"{:.1f}\")\n",
    "# display(styler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# (No correlation table or color_gradient code here; see dedicated cell above)\n",
    "\n",
    "def show_class_balance(data, name=\"data\", labelCol=\"label\"):\n",
    "    \"\"\"Helper function to show class balance based on label.\n",
    "    \n",
    "    Note that this function does not return anything.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): datafame with label\n",
    "        name (str): name to print above metrics for readability \n",
    "        labelCol (str): label column name\n",
    "    \"\"\"\n",
    "\n",
    "    total = data.count()\n",
    "    counts = data.groupBy(labelCol).count().toPandas()\n",
    "    counts[\"ratio\"] = counts[\"count\"] / total\n",
    "\n",
    "    print(f'Class balance [{name}]')\n",
    "    print(f'')\n",
    "    print(f'total:   {total}')\n",
    "    print(f'counts:')\n",
    "    print(counts)\n",
    "    print(f'')\n",
    "\n",
    "    \n",
    "def with_custom_prediction(\n",
    "    pred,\n",
    "    threshold,\n",
    "    probabilityCol=\"probability\",\n",
    "    customPredictionCol=\"customPrediction\",\n",
    "):\n",
    "    \"\"\"Helper function to select a custom prediction column for a custom classification threshold.\n",
    "    \n",
    "    Args:\n",
    "        pred (pyspark.sql.DataFrame): datafame with column for probability \n",
    "        threshold (float): classification threshold\n",
    "        probabilityCol (str): probability column name\n",
    "        customPredictionCol (str): new custom prediction column name\n",
    "    \n",
    "    Returns:\n",
    "        pred (pyspark.sql.DataFrame): dataframe with new colum for custom prediction\n",
    "    \"\"\"\n",
    "\n",
    "    classification_udf = F.udf(lambda x: int(x[1] > threshold), IntegerType())\n",
    "    \n",
    "    return pred.withColumn(customPredictionCol, classification_udf(F.col(probabilityCol)))\n",
    "\n",
    "\n",
    "def show_metrics(\n",
    "    pred,\n",
    "    name=\"data\",\n",
    "    threshold=0.5,\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "):\n",
    "    \"\"\"Helper function to evaluate and show metrics based on a custom classification threshold.\n",
    "    \n",
    "    Note that this function does not return anything.\n",
    "    \n",
    "    Args:\n",
    "        pred (pyspark.sql.DataFrame): datafame with column for probability \n",
    "        name (str): name to print above metrics for readability \n",
    "        threshold (float): classification threshold (default: 0.5)\n",
    "        predictionCol (str): prediction column name\n",
    "        rawPredictionCol (str): raw prediction column name\n",
    "        probabilityCol (str): probability column name\n",
    "    \"\"\"\n",
    "\n",
    "    if threshold != 0.5:\n",
    "\n",
    "        predictionCol = \"customPrediction\"\n",
    "        pred = with_custom_prediction(pred, threshold, probabilityCol=probabilityCol, customPredictionCol=predictionCol)\n",
    "\n",
    "    total = pred.count()\n",
    "\n",
    "    nP_actual = pred.filter((F.col(labelCol) == 1)).count()\n",
    "    nN_actual = pred.filter((F.col(labelCol) == 0)).count()\n",
    "\n",
    "    nP = pred.filter((F.col(predictionCol) == 1)).count()\n",
    "    nN = pred.filter((F.col(predictionCol) == 0)).count()\n",
    "    TP = pred.filter((F.col(predictionCol) == 1) & (F.col(labelCol) == 1)).count()\n",
    "    FP = pred.filter((F.col(predictionCol) == 1) & (F.col(labelCol) == 0)).count()\n",
    "    FN = pred.filter((F.col(predictionCol) == 0) & (F.col(labelCol) == 1)).count()\n",
    "    TN = pred.filter((F.col(predictionCol) == 0) & (F.col(labelCol) == 0)).count()\n",
    "\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = (TP + TN) / total\n",
    "\n",
    "    binary_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol=rawPredictionCol,\n",
    "        labelCol=labelCol,\n",
    "        metricName='areaUnderROC',\n",
    "    )\n",
    "    auroc = binary_evaluator.evaluate(pred)\n",
    "\n",
    "    print(f'Metrics [{name}]')\n",
    "    print(f'')\n",
    "    print(f'threshold: {threshold}')\n",
    "    print(f'')\n",
    "    print(f'total:     {total}')\n",
    "    print(f'')\n",
    "    print(f'nP actual: {nP_actual}')\n",
    "    print(f'nN actual: {nN_actual}')\n",
    "    print(f'')\n",
    "    print(f'nP:        {nP}')\n",
    "    print(f'nN:        {nN}')\n",
    "    print(f'')\n",
    "    print(f'TP         {TP}')\n",
    "    print(f'FP         {FP}')\n",
    "    print(f'FN         {FN}')\n",
    "    print(f'TN         {TN}')\n",
    "    print(f'')\n",
    "    print(f'precision: {precision:.8f}')\n",
    "    print(f'recall:    {recall:.8f}')\n",
    "    print(f'accuracy:  {accuracy:.8f}')\n",
    "    print(f'')\n",
    "    print(f'auroc:     {auroc:.8f}')\n",
    "\n",
    "\n",
    "def expand(x, s=0.05, d=0):\n",
    "    \"\"\"Expand a two element array about its center point by a relative scale or a fixed offset.\n",
    "    Args:\n",
    "        x (list|np.array): two element array\n",
    "        s (float): relative scale to expand array based on its width x[1] - x[0]\n",
    "        d (float): fixed offset to expand array\n",
    "    Returns:\n",
    "        x (np.array): expanded two element array\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(x)\n",
    "    d = d + s * (x[1] - x[0])\n",
    "    \n",
    "    return x + np.array([-d, d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ideal number of partitions\n",
    "\n",
    "conf = sc.getConf()\n",
    "\n",
    "N = int(conf.get(\"spark.executor.instances\"))\n",
    "M = int(conf.get(\"spark.executor.cores\"))\n",
    "partitions = 4 * N * M\n",
    "\n",
    "print(f'ideal # partitions = {partitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "The credit card fraud dataset is stored in HDFS.\n",
    "\n",
    "We will load the dataset and then use `VectorAssembler` to combine the separate feature columns into the single vector column that is expected by most of the classes in the machine learning. \n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data in gzip compressed but Spark will automatically uncompress it as it is loaded.\n",
    "- The data has a header so we can infer schema conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from HDFS\n",
    "\n",
    "fraud = (\n",
    "    spark.read.csv(\"hdfs:///data/fraud/manipulated.csv.gz\", header=True, inferSchema=True)\n",
    "    .repartition(partitions)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "fraud.printSchema()\n",
    "show_as_html(fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select what we need\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col for col in fraud.columns if col.startswith(\"V\")],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(fraud)\n",
    "data = data.select(\n",
    "    F.col('features'),\n",
    "    F.col('Class').alias('label'),\n",
    ")\n",
    "\n",
    "data.printSchema()\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting ###\n",
    "\n",
    "There are several methods for random splitting, but not all of them are as random as might be assumed.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- `randomSplit` does simple random sampling only\n",
    "- `sampleBy` does class specific simple random sampling where the proportion of each class can be changed\n",
    "- `Window` functions can be used to achieve exact stratification if required\n",
    "  - We can also use Python control structures to make it easier to implement complicated logic such as the filtering below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomSplit (simple random sampling, not exact)\n",
    "\n",
    "training, test = data.randomSplit([0.8, 0.2])\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample by (per-class simple random sampling, which is still not exact)\n",
    "\n",
    "temp = data.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "training = temp.sampleBy(\"label\", fractions={0: 0.8, 1: 0.8})\n",
    "training.cache()\n",
    "\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "\n",
    "training = training.drop(\"id\")\n",
    "test = test.drop(\"id\")\n",
    "\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact stratification using Window (hard coded for this binary example only)\n",
    "\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"random\", F.rand())\n",
    "    .withColumn(\"row\", F.row_number().over(Window.partitionBy(\"label\").orderBy(\"random\")))\n",
    ")\n",
    "\n",
    "training = temp\n",
    "training = training.where((F.col(\"label\") != 0) | (F.col(\"row\") < 284315 * 0.8))\n",
    "training = training.where((F.col(\"label\") != 1) | (F.col(\"row\") < 492    * 0.8))\n",
    "\n",
    "training.cache()\n",
    "\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "\n",
    "training = training.drop(\"id\", \"random\", \"row\")\n",
    "test = test.drop(\"id\", \"random\", \"row\")\n",
    "\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact stratification using Window (multi-class, counts computed automatically)\n",
    "\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"random\", F.rand())\n",
    "    .withColumn(\"row\", F.row_number().over(Window.partitionBy(\"label\").orderBy(\"random\")))\n",
    ")\n",
    "\n",
    "counts = {label: count for label, count in data.groupBy(\"label\").count().collect()}\n",
    "training = temp\n",
    "for label, count in counts.items():\n",
    "    training = training.where((F.col(\"label\") != label) | (F.col(\"row\") < count * 0.8))\n",
    "\n",
    "training.cache()\n",
    "\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "\n",
    "training = training.drop(\"id\", \"random\", \"row\")\n",
    "test = test.drop(\"id\", \"random\", \"row\")\n",
    "\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact stratification using Window (multi-class, using percent rank)\n",
    "\n",
    "temp = (\n",
    "    data\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"random\", F.rand())\n",
    "    .withColumn(\"rank\", F.percent_rank().over(Window.partitionBy(\"label\").orderBy(\"random\")))\n",
    ")\n",
    "\n",
    "training = temp\n",
    "for label, count in counts.items():\n",
    "    training = training.where((F.col(\"label\") != label) | (F.col(\"rank\") < 0.8))\n",
    "\n",
    "training.cache()\n",
    "\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "\n",
    "training = training.drop(\"id\", \"random\", \"row\")\n",
    "test = test.drop(\"id\", \"random\", \"row\")\n",
    "\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
