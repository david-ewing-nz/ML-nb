{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud ###\n",
    "\n",
    "The credit card fraud dataset is relatively simple, with numeric features that have been anonymised using PCA. There is however significant class imbalance with only 492 examples of fraud out of 284,807 transactions in total. This requires careful handling and makes evaluating the performance of the model hard.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Data processing](#Data-processing)\n",
    "- [Training](#Training)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- You can use `sc.getConf()` to calculate the ideal number of partitions for the resources you have allocated.\n",
    "- We can write functions to extract snippets of code that we want to use more than once and we can customize their behaviour with keyword arguments.\n",
    "  - `with_custom_prediction(pred, threshold)`\n",
    "  - `show_class_balance(data, name)`\n",
    "  - `show_metrics(pred, threshold)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=4, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark imports\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports to be used locally\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(edgeitems=5, threshold=100, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a blue-white-red colour gradient to a DataFrame\n",
    "def color_gradient(val, vmin=-1, vmax=1):\n",
    "    \"\"\"Return background colour for a cell based on value, using a blue-white-red gradient.\n",
    "    Dark blue for 1, white for 0, dark red for -1.\"\"\"\n",
    "    from matplotlib import colors\n",
    "    norm = colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = colors.LinearSegmentedColormap.from_list('', ['red', 'white', 'blue'])\n",
    "    rgb = cmap(norm(val))[:3]\n",
    "    return f'background-color: rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def show_class_balance(data, name=\"data\", labelCol=\"label\"):\n",
    "    \"\"\"Helper function to show class balance based on label.\n",
    "    \n",
    "    Note that this function does not return anything.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): datafame with label\n",
    "        name (str): name to print above metrics for readability \n",
    "        labelCol (str): label column name\n",
    "    \"\"\"\n",
    "\n",
    "    total = data.count()\n",
    "    counts = data.groupBy(labelCol).count().toPandas()\n",
    "    counts[\"ratio\"] = counts[\"count\"] / total\n",
    "\n",
    "    print(f'Class balance [{name}]')\n",
    "    print(f'')\n",
    "    print(f'total:   {total}')\n",
    "    print(f'counts:')\n",
    "    print(counts)\n",
    "    print(f'')\n",
    "\n",
    "    \n",
    "def with_custom_prediction(\n",
    "    pred,\n",
    "    threshold,\n",
    "    probabilityCol=\"probability\",\n",
    "    customPredictionCol=\"customPrediction\",\n",
    "):\n",
    "    \"\"\"Helper function to select a custom prediction column for a custom classification threshold.\n",
    "    \n",
    "    Args:\n",
    "        pred (pyspark.sql.DataFrame): datafame with column for probability \n",
    "        threshold (float): classification threshold\n",
    "        probabilityCol (str): probability column name\n",
    "        customPredictionCol (str): new custom prediction column name\n",
    "    \n",
    "    Returns:\n",
    "        pred (pyspark.sql.DataFrame): dataframe with new colum for custom prediction\n",
    "    \"\"\"\n",
    "\n",
    "    classification_udf = F.udf(lambda x: int(x[1] > threshold), IntegerType())\n",
    "    \n",
    "    return pred.withColumn(customPredictionCol, classification_udf(F.col(probabilityCol)))\n",
    "\n",
    "\n",
    "def show_metrics(\n",
    "    pred,\n",
    "    name=\"data\",\n",
    "    threshold=0.5,\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "):\n",
    "    \"\"\"Helper function to evaluate and show metrics based on a custom classification threshold.\n",
    "    \n",
    "    Note that this function does not return anything.\n",
    "    \n",
    "    Args:\n",
    "        pred (pyspark.sql.DataFrame): datafame with column for probability \n",
    "        name (str): name to print above metrics for readability \n",
    "        threshold (float): classification threshold (default: 0.5)\n",
    "        predictionCol (str): prediction column name\n",
    "        rawPredictionCol (str): raw prediction column name\n",
    "        probabilityCol (str): probability column name\n",
    "    \"\"\"\n",
    "\n",
    "    if threshold != 0.5:\n",
    "\n",
    "        predictionCol = \"customPrediction\"\n",
    "        pred = with_custom_prediction(pred, threshold, probabilityCol=probabilityCol, customPredictionCol=predictionCol)\n",
    "\n",
    "    total = pred.count()\n",
    "\n",
    "    nP_actual = pred.filter((F.col(labelCol) == 1)).count()\n",
    "    nN_actual = pred.filter((F.col(labelCol) == 0)).count()\n",
    "\n",
    "    nP = pred.filter((F.col(predictionCol) == 1)).count()\n",
    "    nN = pred.filter((F.col(predictionCol) == 0)).count()\n",
    "    TP = pred.filter((F.col(predictionCol) == 1) & (F.col(labelCol) == 1)).count()\n",
    "    FP = pred.filter((F.col(predictionCol) == 1) & (F.col(labelCol) == 0)).count()\n",
    "    FN = pred.filter((F.col(predictionCol) == 0) & (F.col(labelCol) == 1)).count()\n",
    "    TN = pred.filter((F.col(predictionCol) == 0) & (F.col(labelCol) == 0)).count()\n",
    "\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = (TP + TN) / total\n",
    "\n",
    "    binary_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol=rawPredictionCol,\n",
    "        labelCol=labelCol,\n",
    "        metricName='areaUnderROC',\n",
    "    )\n",
    "    auroc = binary_evaluator.evaluate(pred)\n",
    "\n",
    "    print(f'Metrics [{name}]')\n",
    "    print(f'')\n",
    "    print(f'threshold: {threshold}')\n",
    "    print(f'')\n",
    "    print(f'total:     {total}')\n",
    "    print(f'')\n",
    "    print(f'nP actual: {nP_actual}')\n",
    "    print(f'nN actual: {nN_actual}')\n",
    "    print(f'')\n",
    "    print(f'nP:        {nP}')\n",
    "    print(f'nN:        {nN}')\n",
    "    print(f'')\n",
    "    print(f'TP         {TP}')\n",
    "    print(f'FP         {FP}')\n",
    "    print(f'FN         {FN}')\n",
    "    print(f'TN         {TN}')\n",
    "    print(f'')\n",
    "    print(f'precision: {precision:.8f}')\n",
    "    print(f'recall:    {recall:.8f}')\n",
    "    print(f'accuracy:  {accuracy:.8f}')\n",
    "    print(f'')\n",
    "    print(f'auroc:     {auroc:.8f}')\n",
    "\n",
    "\n",
    "def expand(x, s=0.05, d=0):\n",
    "    \"\"\"Expand a two element array about its center point by a relative scale or a fixed offset.\n",
    "    Args:\n",
    "        x (list|np.array): two element array\n",
    "        s (float): relative scale to expand array based on its width x[1] - x[0]\n",
    "        d (float): fixed offset to expand array\n",
    "    Returns:\n",
    "        x (np.array): expanded two element array\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(x)\n",
    "    d = d + s * (x[1] - x[0])\n",
    "    \n",
    "    return x + np.array([-d, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ideal number of partitions\n",
    "\n",
    "conf = sc.getConf()\n",
    "\n",
    "N = int(conf.get(\"spark.executor.instances\"))\n",
    "M = int(conf.get(\"spark.executor.cores\"))\n",
    "partitions = 4 * N * M\n",
    "\n",
    "print(f'ideal # partitions = {partitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "The credit card fraud dataset is stored in HDFS.\n",
    "\n",
    "We will load the dataset and then use `VectorAssembler` to combine the separate feature columns into the single vector column that is expected by most of the classes in the machine learning. \n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data in gzip compressed but Spark will automatically uncompress it as it is loaded.\n",
    "- The data has a header so we can infer schema conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from HDFS\n",
    "\n",
    "fraud = (\n",
    "    spark.read.csv(\"hdfs:///data/fraud/manipulated.csv.gz\", header=True, inferSchema=True)\n",
    "    .repartition(partitions)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "fraud.printSchema()\n",
    "show_as_html(fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select what we need\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col for col in fraud.columns if col.startswith(\"V\")],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(fraud)\n",
    "data = data.select(\n",
    "    F.col('features'),\n",
    "    F.col('Class').alias('label'),\n",
    ")\n",
    "\n",
    "data.printSchema()\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing ###\n",
    "\n",
    "The data is well structured but we should verify our assumptions and explore the relationships between the variables before fitting a model.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The features in 2013 were generated by PCA but have been manipulated to introduce random noise and correlations.\n",
    "- We can take advantage of some `pandas` functionality to present the descriptive statistics from `.describe()` in a more readable way.\n",
    "- We can use the `display` and `HTML` functions from `IPython.display` to customize how the correlations are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics\n",
    "\n",
    "statistics = (\n",
    "    fraud\n",
    "    .describe()\n",
    "    .toPandas()\n",
    "    .set_index(\"summary\")\n",
    "    .rename_axis(None)\n",
    "    .T\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "display(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class frequency\n",
    "\n",
    "show_as_html(data.groupby('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore correlations in case there are features that are highly correlated\n",
    "\n",
    "correlations = Correlation.corr(data, 'features', 'pearson')\n",
    "\n",
    "show_as_html(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect correlations locally and convert to numpy array\n",
    "\n",
    "correlations_local = correlations.collect()[0][0].toArray()\n",
    "\n",
    "display(pd.DataFrame(correlations_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round correlations and display with blue-white-red colour gradient\n",
    "df_corr = pd.DataFrame(correlations_local)\n",
    "df_corr_rounded = df_corr.round(1)\n",
    "styler = df_corr_rounded.style.applymap(color_gradient)\n",
    "display(styler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling ###\n",
    "\n",
    "We can easily train and evaluate a baseline model using the `LogisticRegression` class and the helper function `show_metrics`.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- We will explore different splitting and training strategies in the other examples in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training\n",
    "\n",
    "training, test = data.randomSplit([0.8, 0.2])\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "show_class_balance(data, \"data\")\n",
    "show_class_balance(training, \"training\")\n",
    "show_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate metrics\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(training)\n",
    "\n",
    "pred = lr_model.transform(test)\n",
    "pred.cache()\n",
    "\n",
    "show_metrics(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
